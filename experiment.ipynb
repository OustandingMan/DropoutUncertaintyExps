{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab-only setup. No need to run this cell in other environments.\n",
    "\n",
    "# Mount my Google Drive root folder\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# cd to assignment1 directory\n",
    "%cd 'drive/My Drive/Colab Notebooks/DropoutUncertaintyExps'\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using plaidml.keras.backend backend.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "# NN is defined in net/net.py\n",
    "import net.net as net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment settings\n",
    "data_directory = \"energy\"\n",
    "epochs_multiplier = \"10\"\n",
    "num_hidden_layers = \"2\"\n",
    "results_file_directory_original = False\n",
    "\n",
    "# We fix the random seed\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_DATA_DIRECTORY_PATH = \"./UCI_Datasets/\" + data_directory + \"/data/\"\n",
    "_DROPOUT_RATES_FILE = _DATA_DIRECTORY_PATH + \"dropout_rates.txt\"\n",
    "_TAU_VALUES_FILE = _DATA_DIRECTORY_PATH + \"tau_values.txt\"\n",
    "_DATA_FILE = _DATA_DIRECTORY_PATH + \"data.txt\"\n",
    "_HIDDEN_UNITS_FILE = _DATA_DIRECTORY_PATH + \"n_hidden.txt\"\n",
    "_EPOCHS_FILE = _DATA_DIRECTORY_PATH + \"n_epochs.txt\"\n",
    "_INDEX_FEATURES_FILE = _DATA_DIRECTORY_PATH + \"index_features.txt\"\n",
    "_INDEX_TARGET_FILE = _DATA_DIRECTORY_PATH + \"index_target.txt\"\n",
    "_N_SPLITS_FILE = _DATA_DIRECTORY_PATH + \"n_splits.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results file directory setting\n",
    "if results_file_directory_original:\n",
    "    _RESULTS_FILE_ROOT = \"./UCI_Datasets/\" # Original location in experiments.py\n",
    "else:\n",
    "    _RESULTS_FILE_ROOT = (\n",
    "        \"./test_results/\"\n",
    "        + datetime.datetime.today().strftime('%Y%m%d%H%M') + \"/\")\n",
    "\n",
    "_RESULTS_VALIDATION_LL = _RESULTS_FILE_ROOT + data_directory + \"/results/validation_ll_\" + str(epochs_multiplier) + \"_xepochs_\" + str(num_hidden_layers) + \"_hidden_layers.txt\"\n",
    "_RESULTS_VALIDATION_RMSE = _RESULTS_FILE_ROOT + data_directory + \"/results/validation_rmse_\" + str(epochs_multiplier) + \"_xepochs_\" + str(num_hidden_layers) + \"_hidden_layers.txt\"\n",
    "_RESULTS_VALIDATION_MC_RMSE = _RESULTS_FILE_ROOT + data_directory + \"/results/validation_MC_rmse_\" + str(epochs_multiplier) + \"_xepochs_\" + str(num_hidden_layers) + \"_hidden_layers.txt\"\n",
    "\n",
    "_RESULTS_TEST_LL = _RESULTS_FILE_ROOT + data_directory + \"/results/test_ll_\" + str(epochs_multiplier) + \"_xepochs_\" + str(num_hidden_layers) + \"_hidden_layers.txt\"\n",
    "_RESULTS_TEST_TAU = _RESULTS_FILE_ROOT + data_directory + \"/results/test_tau_\" + str(epochs_multiplier) + \"_xepochs_\" + str(num_hidden_layers) + \"_hidden_layers.txt\"\n",
    "_RESULTS_TEST_RMSE = _RESULTS_FILE_ROOT + data_directory + \"/results/test_rmse_\" + str(epochs_multiplier) + \"_xepochs_\" + str(num_hidden_layers) + \"_hidden_layers.txt\"\n",
    "_RESULTS_TEST_MC_RMSE = _RESULTS_FILE_ROOT + data_directory + \"/results/test_MC_rmse_\" + str(epochs_multiplier) + \"_xepochs_\" + str(num_hidden_layers) + \"_hidden_layers.txt\"\n",
    "_RESULTS_TEST_LOG = _RESULTS_FILE_ROOT + data_directory + \"/results/log_\" + str(epochs_multiplier) + \"_xepochs_\" + str(num_hidden_layers) + \"_hidden_layers.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data and other hyperparameters...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print (\"Loading data and other hyperparameters...\")\n",
    "# We load the data\n",
    "\n",
    "data = np.loadtxt(_DATA_FILE)\n",
    "\n",
    "# We load the number of hidden units\n",
    "\n",
    "n_hidden = np.loadtxt(_HIDDEN_UNITS_FILE).tolist()\n",
    "\n",
    "# We load the number of training epocs\n",
    "\n",
    "n_epochs = np.loadtxt(_EPOCHS_FILE).tolist()\n",
    "\n",
    "# We load the indexes for the features and for the target\n",
    "\n",
    "index_features = np.loadtxt(_INDEX_FEATURES_FILE)\n",
    "index_target = np.loadtxt(_INDEX_TARGET_FILE)\n",
    "\n",
    "X = data[ : , [int(i) for i in index_features.tolist()] ]\n",
    "y = data[ : , int(index_target.tolist()) ]\n",
    "\n",
    "# We iterate over the training test splits\n",
    "\n",
    "n_splits = np.loadtxt(_N_SPLITS_FILE)\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors, MC_errors, lls = [], [], []\n",
    "for split in range(int(n_splits)):\n",
    "\n",
    "    # We load the indexes of the training and test sets\n",
    "    print ('Loading file: ' + _get_index_train_test_path(split, train=True))\n",
    "    print ('Loading file: ' + _get_index_train_test_path(split, train=False))\n",
    "    index_train = np.loadtxt(_get_index_train_test_path(split, train=True))\n",
    "    index_test = np.loadtxt(_get_index_train_test_path(split, train=False))\n",
    "\n",
    "    X_train = X[ [int(i) for i in index_train.tolist()] ]\n",
    "    y_train = y[ [int(i) for i in index_train.tolist()] ]\n",
    "    \n",
    "    X_test = X[ [int(i) for i in index_test.tolist()] ]\n",
    "    y_test = y[ [int(i) for i in index_test.tolist()] ]\n",
    "\n",
    "    X_train_original = X_train\n",
    "    y_train_original = y_train\n",
    "    num_training_examples = int(0.8 * X_train.shape[0])\n",
    "    X_validation = X_train[num_training_examples:, :]\n",
    "    y_validation = y_train[num_training_examples:]\n",
    "    X_train = X_train[0:num_training_examples, :]\n",
    "    y_train = y_train[0:num_training_examples]\n",
    "    \n",
    "    # Printing the size of the training, validation and test sets\n",
    "    print ('Number of training examples: ' + str(X_train.shape[0]))\n",
    "    print ('Number of validation examples: ' + str(X_validation.shape[0]))\n",
    "    print ('Number of test examples: ' + str(X_test.shape[0]))\n",
    "    print ('Number of train_original examples: ' + str(X_train_original.shape[0]))\n",
    "\n",
    "    # List of hyperparameters which we will try out using grid-search\n",
    "    dropout_rates = np.loadtxt(_DROPOUT_RATES_FILE).tolist()\n",
    "    tau_values = np.loadtxt(_TAU_VALUES_FILE).tolist()\n",
    "\n",
    "    # We perform grid-search to select the best hyperparameters based on the highest log-likelihood value\n",
    "    best_network = None\n",
    "    best_ll = -float('inf')\n",
    "    best_tau = 0\n",
    "    best_dropout = 0\n",
    "\n",
    "    for dropout_rate in dropout_rates:\n",
    "        for tau in tau_values:\n",
    "            print ('Grid search step: Tau: ' + str(tau) + ' Dropout rate: ' + str(dropout_rate))\n",
    "            network = net.net(X_train, y_train, ([ int(n_hidden) ] * num_hidden_layers),\n",
    "                    normalize = True, n_epochs = int(n_epochs * epochs_multiplier), tau = tau,\n",
    "                    dropout = dropout_rate)\n",
    "\n",
    "            # We obtain the test RMSE and the test ll from the validation sets\n",
    "\n",
    "            error, MC_error, ll = network.predict(X_validation, y_validation)\n",
    "            if (ll > best_ll):\n",
    "                best_ll = ll\n",
    "                best_network = network\n",
    "                best_tau = tau\n",
    "                best_dropout = dropout_rate\n",
    "                print ('Best log_likelihood changed to: ' + str(best_ll))\n",
    "                print ('Best tau changed to: ' + str(best_tau))\n",
    "                print ('Best dropout rate changed to: ' + str(best_dropout))\n",
    "            \n",
    "            # Storing validation results\n",
    "            with open(_RESULTS_VALIDATION_RMSE, \"a\") as myfile:\n",
    "                myfile.write('Dropout_Rate: ' + repr(dropout_rate) + ' Tau: ' + repr(tau) + ' :: ')\n",
    "                myfile.write(repr(error) + '\\n')\n",
    "\n",
    "            with open(_RESULTS_VALIDATION_MC_RMSE, \"a\") as myfile:\n",
    "                myfile.write('Dropout_Rate: ' + repr(dropout_rate) + ' Tau: ' + repr(tau) + ' :: ')\n",
    "                myfile.write(repr(MC_error) + '\\n')\n",
    "\n",
    "            with open(_RESULTS_VALIDATION_LL, \"a\") as myfile:\n",
    "                myfile.write('Dropout_Rate: ' + repr(dropout_rate) + ' Tau: ' + repr(tau) + ' :: ')\n",
    "                myfile.write(repr(ll) + '\\n')\n",
    "\n",
    "    # Storing test results\n",
    "    best_network = net.net(X_train_original, y_train_original, ([ int(n_hidden) ] * num_hidden_layers),\n",
    "                    normalize = True, n_epochs = int(n_epochs * epochs_multiplier), tau = best_tau,\n",
    "                    dropout = best_dropout)\n",
    "    error, MC_error, ll = best_network.predict(X_test, y_test)\n",
    "    \n",
    "    with open(_RESULTS_TEST_RMSE, \"a\") as myfile:\n",
    "        myfile.write(repr(error) + '\\n')\n",
    "\n",
    "    with open(_RESULTS_TEST_MC_RMSE, \"a\") as myfile:\n",
    "        myfile.write(repr(MC_error) + '\\n')\n",
    "\n",
    "    with open(_RESULTS_TEST_LL, \"a\") as myfile:\n",
    "        myfile.write(repr(ll) + '\\n')\n",
    "\n",
    "    with open(_RESULTS_TEST_TAU, \"a\") as myfile:\n",
    "        myfile.write(repr(best_network.tau) + '\\n')\n",
    "\n",
    "    print (\"Tests on split \" + str(split) + \" complete.\")\n",
    "    errors += [error]\n",
    "    MC_errors += [MC_error]\n",
    "    lls += [ll]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(_RESULTS_TEST_LOG, \"a\") as myfile:\n",
    "    myfile.write('errors %f +- %f (stddev) +- %f (std error), median %f 25p %f 75p %f \\n' % (\n",
    "        np.mean(errors), np.std(errors), np.std(errors)/math.sqrt(n_splits),\n",
    "        np.percentile(errors, 50), np.percentile(errors, 25), np.percentile(errors, 75)))\n",
    "    myfile.write('MC errors %f +- %f (stddev) +- %f (std error), median %f 25p %f 75p %f \\n' % (\n",
    "        np.mean(MC_errors), np.std(MC_errors), np.std(MC_errors)/math.sqrt(n_splits),\n",
    "        np.percentile(MC_errors, 50), np.percentile(MC_errors, 25), np.percentile(MC_errors, 75)))\n",
    "    myfile.write('lls %f +- %f (stddev) +- %f (std error), median %f 25p %f 75p %f \\n' % (\n",
    "        np.mean(lls), np.std(lls), np.std(lls)/math.sqrt(n_splits), \n",
    "        np.percentile(lls, 50), np.percentile(lls, 25), np.percentile(lls, 75)))"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
